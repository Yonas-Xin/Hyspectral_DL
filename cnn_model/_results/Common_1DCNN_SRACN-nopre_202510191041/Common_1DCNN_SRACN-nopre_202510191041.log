2025-10-19 10:42:11 Epoch[  1/300]	Train-Loss: 1.398106 (1.732173)	Acc: 48.8889 (45.8557)	Test-Loss: 2.887646 (2.264542)	Acc: 0.0000 (24.0000)	Lr: 3.00e-04
2025-10-19 10:42:16 Epoch[  2/300]	Train-Loss: 0.967947 (1.184888)	Acc: 71.1111 (57.6029)	Test-Loss: 3.345395 (1.425289)	Acc: 0.0000 (53.9556)	Lr: 3.00e-04
2025-10-19 10:42:19 Epoch[  3/300]	Train-Loss: 0.710121 (0.857718)	Acc: 80.0000 (68.9326)	Test-Loss: 2.057417 (0.794768)	Acc: 0.0000 (73.1556)	Lr: 3.00e-04
2025-10-19 10:42:23 Epoch[  4/300]	Train-Loss: 0.621957 (0.699873)	Acc: 75.5556 (75.9094)	Test-Loss: 2.034420 (1.099143)	Acc: 0.0000 (63.3778)	Lr: 3.00e-04
2025-10-19 10:42:27 Epoch[  5/300]	Train-Loss: 0.474771 (0.587680)	Acc: 77.7778 (78.7716)	Test-Loss: 1.792508 (0.682508)	Acc: 14.2857 (75.0222)	Lr: 3.00e-04
2025-10-19 10:42:31 Epoch[  6/300]	Train-Loss: 0.457706 (0.518746)	Acc: 82.2222 (81.1568)	Test-Loss: 1.589361 (0.601190)	Acc: 33.3333 (79.2889)	Lr: 3.00e-04
2025-10-19 10:42:35 Epoch[  7/300]	Train-Loss: 0.575837 (0.434135)	Acc: 84.4445 (83.7209)	Test-Loss: 1.757319 (0.729252)	Acc: 33.3333 (72.0889)	Lr: 3.00e-04
2025-10-19 10:42:39 Epoch[  8/300]	Train-Loss: 0.244520 (0.395590)	Acc: 91.1111 (85.9869)	Test-Loss: 2.034262 (0.521980)	Acc: 19.0476 (81.7778)	Lr: 3.00e-04
2025-10-19 10:42:43 Epoch[  9/300]	Train-Loss: 0.327810 (0.383616)	Acc: 86.6667 (85.9272)	Test-Loss: 1.316782 (0.756853)	Acc: 71.4286 (74.2222)	Lr: 3.00e-04
2025-10-19 10:42:47 Epoch[ 10/300]	Train-Loss: 0.307580 (0.355676)	Acc: 88.8889 (86.7621)	Test-Loss: 0.775536 (0.497846)	Acc: 71.4286 (83.2889)	Lr: 3.00e-04
2025-10-19 10:42:51 Epoch[ 11/300]	Train-Loss: 0.217630 (0.319629)	Acc: 91.1111 (87.8950)	Test-Loss: 1.492934 (0.476992)	Acc: 38.0952 (83.4667)	Lr: 3.00e-04
2025-10-19 10:42:55 Epoch[ 12/300]	Train-Loss: 0.088713 (0.275030)	Acc: 97.7778 (90.2803)	Test-Loss: 3.503556 (0.617417)	Acc: 9.5238 (79.7333)	Lr: 3.00e-04
2025-10-19 10:43:01 Model saved at Epoch 11. The best training_acc:87.8950%. The best testing_acc:83.4667%.


2025-10-19 10:43:01 Test_acc: 0.8347. Cohen's Kappa: 0.8066
Classification Report:
              precision    recall  f1-score   support

           0     0.8582    0.8485    0.8533       264
           1     0.8549    0.8450    0.8499       258
           2     0.8585    0.8053    0.8311       113
           3     0.9412    0.8767    0.9078        73
           4     0.5914    0.8209    0.6875        67
           5     0.7759    0.6716    0.7200        67
           6     0.8621    0.8333    0.8475       120
           7     0.9828    1.0000    0.9913        57
           8     0.8649    1.0000    0.9275        32
           9     0.6721    0.8542    0.7523        48
          10     1.0000    0.4615    0.6316        26

    accuracy                         0.8347      1125
   macro avg     0.8420    0.8197    0.8182      1125
weighted avg     0.8443    0.8347    0.8351      1125

Confusion Matrix:
[[224,   6,   0,   4,   8,   4,   1,   0,   3,  14,   0],
 [  2, 218,  15,   0,   8,   1,  10,   1,   0,   3,   0],
 [  0,  18,  91,   0,   4,   0,   0,   0,   0,   0,   0],
 [  6,   0,   0,  64,   2,   1,   0,   0,   0,   0,   0],
 [ 11,   0,   0,   0,  55,   1,   0,   0,   0,   0,   0],
 [  6,   0,   0,   0,  10,  45,   5,   0,   1,   0,   0],
 [  1,  10,   0,   0,   3,   6, 100,   0,   0,   0,   0],
 [  0,   0,   0,   0,   0,   0,   0,  57,   0,   0,   0],
 [  0,   0,   0,   0,   0,   0,   0,   0,  32,   0,   0],
 [  4,   2,   0,   0,   0,   0,   0,   0,   1,  41,   0],
 [  7,   1,   0,   0,   3,   0,   0,   0,   0,   3,  12]]
